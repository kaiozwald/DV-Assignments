{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiozwald/DV-Assignments/blob/main/Copy_of_music_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EB4ftsyhBDk"
      },
      "source": [
        "***N.B. this notebook has been tested in Google Colab***\n",
        "\n",
        "# Music Genre Classification\n",
        "\n",
        "In this project, you will build a machine learning algorithm to classify music genres using audio features. Starting with the provided dataset, your task is to develop a model that effectively solves this multiclass classification problem. Use the baseline notebook as a starting point and improve upon it.\n",
        "\n",
        "***Overall Goal: to design a complete pipeline that improves accuracy from the current 37% to at least 70%, ideally approaching 80%***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTVG0TRmazpB"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this project, you will work with a dataset of music samples from various genres. The dataset has been purposely left a bit messy, with some entries missing labels and others containing empty audio files. To start with, your task is to clean and explore the dataset, turning it into a well-organized resource for analysis.\n",
        "\n",
        "This notebook includes a basic, \"weak\" baseline to get you started. It serves as a simple starting point, but it is neither thorough nor accurate. You are expected to build upon it, applying your own strategies to improve the data science pipeline (including data cleaning, curation, feature engineering, etc) before moving into model building, parameters tuning, and model evaluation.\n",
        "\n",
        "The formal details of the assignment are provided at the end of the notebook. To start with, focus on understanding the dataset and planning your strategies to tackle its challenges.\n",
        "\n",
        "**We expect you to submit a modified version of this notebook with your improvements. Please download a copy of this assignment in your private Python programming environment, before making any changes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQqFo3t-bix6"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jBR6dB4g01p"
      },
      "source": [
        "Let's install all required dependencies:\n",
        "\n",
        "- **datasets**: Access to large-scale datasets.\n",
        "- **librosa**: Tools for audio analysis.\n",
        "- **pandas** & **numpy**: Tabular data manipulation and numerical operations.\n",
        "- **scikit-learn**: Machine learning algorithms and tools.\n",
        "- **tqdm**: Progress bar.\n",
        "\n",
        "You might be familiar with most of these already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmucrCDx6aZN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets==3.5.0 librosa pandas numpy scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5QDV6Wig85e"
      },
      "source": [
        "And import the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKYw6B3g6pPm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from datasets import load_dataset\n",
        "from IPython.display import Audio, display\n",
        "from tqdm import tqdm_notebook as tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKxZKHnjeM6l"
      },
      "outputs": [],
      "source": [
        "# Include here any additional modules that you might need\n",
        "# ...\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import SelectKBest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCFUq7TyWUT3"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The dataset consists of music samples from various genres, including:\n",
        "\n",
        "- **Genres**: `Blues`, `Classical`, `Country`, `Disco`, `HipHop`, `Jazz`, `Metal`, `Pop`, `Reggae`, and `Rock`.\n",
        "\n",
        "The dataset is a bit messy and includes some **unlabeled data** and **empty audio files**. We have provided basic preprocessing, but more in-depth data cleaning, feature extraction, and preparation will be a part of your assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw0SVWUYhLBO"
      },
      "source": [
        "Let's download the audio dataset using the Hugging Face datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzKXSQs36qV4"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"unibz-ds-course/audio_assignment\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXY6iDBWwRwN"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aji8xId4qFe"
      },
      "outputs": [],
      "source": [
        "print(f\"Num of samples in the dataset: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ajqJ4-ZVvq"
      },
      "source": [
        "Let's take a glance at a sample from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4um4LfjAa0l3"
      },
      "outputs": [],
      "source": [
        "entry = dataset[10]\n",
        "\n",
        "audio_array = entry['audio']['array']\n",
        "sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "print(f\"Element: {entry}\")\n",
        "print(f\"File Path: {entry['file']}\")\n",
        "print(f\"Number of Samples: {len(audio_array)}\")\n",
        "print(f\"Sampling Rate: {sampling_rate} Hz\")\n",
        "\n",
        "audio_length_seconds = len(audio_array) / sampling_rate\n",
        "print(f\"Audio Length: {audio_length_seconds:.2f} seconds\")\n",
        "\n",
        "genre_id = entry['genre']\n",
        "genre_label = dataset.features['genre'].int2str(genre_id)\n",
        "print(f\"Genre (ID): {genre_id}\")\n",
        "print(f\"Genre (Label): {genre_label}\")\n",
        "\n",
        "display(Audio(audio_array, rate=sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qW7b3HLeoEq"
      },
      "source": [
        "**Draw a plot with distribution of the classes (5 points)**\n",
        "\n",
        "Create a visualization (e.g., bar chart or histogram) that shows how many samples belong to each class.\n",
        "This helps identify whether the dataset is balanced or if some classes are underrepresented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQeAMy9l2w7l"
      },
      "outputs": [],
      "source": [
        "# write code to draw the distribution\n",
        "genre = []\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "  entry = dataset[i]\n",
        "  genre_id = entry['genre']\n",
        "  genre_label = dataset.features['genre'].int2str(genre_id) if genre_id is not None else 'Unknown'\n",
        "  genre.append(genre_label)\n",
        "# print(genre)\n",
        "counts_dict = Counter(genre)\n",
        "counts_list = list(counts_dict.values())\n",
        "uniqe_genre = list(set(genre))\n",
        "colors = plt.cm.tab10_r(np.linspace(0, 1, len(uniqe_genre)))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(uniqe_genre, counts_list, color=colors)\n",
        "plt.title('Distribution of num of samples per class')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n",
        "\n",
        "# we can see that generally classes are balanced and there is no inbalance categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQSbN8NXe5c7"
      },
      "source": [
        "**Draw distribution of lengths of audios (5 points)**\n",
        "\n",
        "Plot the distribution of audio lengths in the dataset to analyze how durations vary across samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC6ufj2m2s-O"
      },
      "outputs": [],
      "source": [
        "# write code to draw the distribution\n",
        "audio_len_arr = []\n",
        "for i in range(len(dataset)):\n",
        "  entry = dataset[i]\n",
        "  audio_array = entry['audio']['array']\n",
        "  sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "  audio_length_seconds = len(audio_array) / sampling_rate\n",
        "  audio_len_arr.append(audio_length_seconds)\n",
        "\n",
        "plt.figure(figsize=(8, 5),)\n",
        "plt.hist(audio_len_arr, color='skyblue',)\n",
        "plt.title('Distribution of Audio Lengths per sample')\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n",
        "\n",
        "# we can notice that audio length in general are between (29.9 - 30.1) and they are common between 500+ samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrhiNmjPglH4"
      },
      "source": [
        "**Delete empty samples (5 points)**\n",
        "\n",
        "Implement the function to remove empty samples (audios with silence only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQj6v297greH"
      },
      "outputs": [],
      "source": [
        "def filter_empty_samples(entry):\n",
        "    threshold = 1e-6\n",
        "    audio = entry['audio']['array']\n",
        "    return any(abs(sample) > threshold for sample in audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf73y3TMhJK7"
      },
      "outputs": [],
      "source": [
        "filtered_dataset = dataset.filter(filter_empty_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmf3nwvzmiUn"
      },
      "outputs": [],
      "source": [
        "assert len(filtered_dataset) == 970, \"Your filtering function is wrong\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5HPxWYoJHqI"
      },
      "source": [
        "Uncomment the code above. If the assertion fails, please check your function for bugs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCMjE5UYlRPu"
      },
      "source": [
        "**Delete unlabeled samples (5 points)**\n",
        "\n",
        "Implement the function to remove unlabeled samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ROox7StlV6N"
      },
      "outputs": [],
      "source": [
        "def filter_unlabeled_samples(entry):\n",
        "  return entry['genre'] is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpGekQyDlk5o"
      },
      "outputs": [],
      "source": [
        "filtered_dataset = filtered_dataset.filter(filter_unlabeled_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3abNWE7glwI4"
      },
      "outputs": [],
      "source": [
        "assert len(filtered_dataset) == 848, \"Your filtering function is wrong\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuK76xWTJSFh"
      },
      "source": [
        "Uncomment the code above. If the assertion fails, please check your function for bugs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2-AKqdvv-Z"
      },
      "source": [
        "Now we can extract some features from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmfDsReXmFCE"
      },
      "source": [
        "### **Mel Frequency Cepstral Coefficients**\n",
        "\n",
        "**[Mel Frequency Cepstral Coefficients (MFCCs)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)** are commonly used in audio analysis to capture key features of sound. They help represent the important characteristics of an audio signal, making them ideal for tasks like music genre classification and speech recognition.\n",
        "\n",
        "We're not going to dive deep into the complex details of audio processing, but it's useful to know that MFCCs help simplify raw audio data while retaining important information.\n",
        "\n",
        "#### Basic Steps in MFCC Extraction:\n",
        "1. **Frequency Domain Conversion**: The audio signal is split into short frames, and we apply the Fourier Transform to convert them from the time domain to the frequency domain.\n",
        "2. **Mel Scale Mapping**: The frequency spectrum is converted to the Mel scale, which better represents how humans perceive sound, emphasizing lower frequencies.\n",
        "3. **Logarithm and DCT**: After mapping to the Mel scale, we apply a logarithm and the Discrete Cosine Transform (DCT) to get the MFCCs. These summarize the \"cepstral\" information of the audio signal.\n",
        "\n",
        "The parameter `n_mfcc` controls **how many MFCC coefficients** are extracted for each frame. For example, setting `n_mfcc=8` means we extract 8 coefficients, where lower coefficients capture broad audio features, and higher coefficients capture the more finer details.\n",
        "\n",
        "#### Why MFCCs Are Important:\n",
        "MFCCs help capture the **tonal quality** of the sound and reduce the complexity of the raw audio signal. By summarizing the audio into a smaller set of features, they allow machine learning models to classify and recognize different types of sounds more effectively.\n",
        "\n",
        "In this notebook, we'll use the **mean** and **variance** of the MFCCs over time to create a robust feature set for our classification model. Adjusting the `n_mfcc` parameter allows us to control the number of features extracted for each audio sample.\n",
        "\n",
        "#### **Additional Features**\n",
        "Consider exploring additional audio features to enhance your model's performance. There are various acoustic properties you could extract from the audio signals, such as zero crossings, harmonic-percussive separation, tempo, spectral centroids, spectral rolloff, chromagram, RMS energy, spectral bandwidth, etc. When working with these features, it's often useful to compute summary statistics like the mean and variance across the audio sample. These summary statistics can capture the overall characteristics and variability of the feature, reducing the dimensionality of your data while retaining important information. Experimenting with these features and their statistical summaries could potentially improve your model's accuracy and robustness in distinguishing between different audio characteristics.\n",
        "\n",
        "#### **Feature Analysis**\n",
        "Don´t forget to optimize the use of features, identifying and handling irrelevant and reduntant features. Then use feature ranking to identify which features are more influential, and evaluate quantitatively how many top-features to retain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0ZT0y2OZjlD"
      },
      "outputs": [],
      "source": [
        "# func to extract mfcc features\n",
        "# describes how the sound sounds (tone/character)\n",
        "\n",
        "def extract_mfcc_features(dataset, n_mfcc):\n",
        "    mfcc_features = []\n",
        "\n",
        "    # here we might have used Dataset.map method, unfortunately, it consumes extra memory and runs out of RAM in colab\n",
        "    for entry in tqdm(dataset, desc=\"Extracting MFCC Features\"):\n",
        "        audio_array = entry['audio']['array']\n",
        "        sampling_rate = entry['audio']['sampling_rate']\n",
        "\n",
        "        mfcc = librosa.feature.mfcc(y=audio_array, sr=sampling_rate, n_mfcc=n_mfcc)\n",
        "\n",
        "        mfcc_mean = np.mean(mfcc, axis=1)\n",
        "        mfcc_var = np.var(mfcc, axis=1)\n",
        "\n",
        "        feature_dict = {}\n",
        "\n",
        "        for i in range(n_mfcc):\n",
        "            feature_dict[f'mfcc_mean{i+1}'] = mfcc_mean[i]\n",
        "\n",
        "        for i in range(n_mfcc):\n",
        "            feature_dict[f'mfcc_var{i+1}'] = mfcc_var[i]\n",
        "\n",
        "        feature_dict['genre'] = entry['genre']\n",
        "\n",
        "        mfcc_features.append(feature_dict)\n",
        "\n",
        "    return mfcc_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgKPQSV9gZl2"
      },
      "source": [
        "Let's take a look at the output of the function. We will pass there just 2 samples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXXP3V05gNaN"
      },
      "outputs": [],
      "source": [
        "extract_mfcc_features(filtered_dataset.select(range(2)), n_mfcc=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oYggNQEkgoN"
      },
      "source": [
        "The function generates `n_mfcc * 2` features for each sample. Consider analyzing their correlation with a matrix and experimenting with different `n_mfcc` values to observe how feature relationships change. While MFCC features are effective for audio analysis, you might also improve performance by incorporating additional features such as RMS or Spectral Contrast. Once you've explored these options, proceed to training the model using the extracted features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTci3PV8zF7D"
      },
      "source": [
        "**Implement functions to extract RMS and Spectral Contrast (10 points in total)**\n",
        "\n",
        "Write functions to extract these features, also add their description and discuss why they might be useful in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK-HdKs2z7jY"
      },
      "outputs": [],
      "source": [
        "# func for extracting rms\n",
        "# shows how loud the sound is\n",
        "def extract_rms(entry):\n",
        "    audio = entry[\"audio\"][\"array\"]\n",
        "\n",
        "    rms = librosa.feature.rms(y=audio)\n",
        "    rms_mean = float(np.mean(rms))\n",
        "    rms_var = float(np.var(rms))\n",
        "\n",
        "    return {\n",
        "        \"rms_mean\": rms_mean,\n",
        "        \"rms_var\": rms_var\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbzHyQStzvKq"
      },
      "outputs": [],
      "source": [
        "# func for extracting spectral contrast\n",
        "# shows sharp vs smooth parts of sound\n",
        "\n",
        "def extract_spectral_contrast(entry):\n",
        "    audio = entry[\"audio\"][\"array\"]\n",
        "    sr = entry[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "    contrast_mean = np.mean(contrast, axis=1)\n",
        "    contrast_var = np.var(contrast, axis=1)\n",
        "\n",
        "    features = {}\n",
        "    for i in range(contrast.shape[0]):\n",
        "        features[f\"contrast_mean{i+1}\"] = contrast_mean[i]\n",
        "        features[f\"contrast_var{i+1}\"] = contrast_var[i]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug7JJ9Kht8Ou"
      },
      "source": [
        "**Explore and add other features useful for classification (10 points)**\n",
        "\n",
        "It can be in a single function or in separated functions. Select some features and provide a description for each one. You can choose as many features as you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IULgC3Fduki9"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "\n",
        "# func for extracting zcr\n",
        "# how often the sound changes sign (roughness)\n",
        "def extract_zcr(entry):\n",
        "  audio = entry[\"audio\"][\"array\"]\n",
        "  zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
        "  zcr_mean = float(np.mean(zcr))\n",
        "  zcr_var = float(np.var(zcr))\n",
        "  return {\n",
        "      \"zcr_mean\":zcr_mean,\n",
        "      \"zcr_var\":zcr_var}\n",
        "\n",
        "# func to extract hpss\n",
        "# splits melody and beats\n",
        "def extract_hpss_single(entry):\n",
        "    audio = entry[\"audio\"][\"array\"]\n",
        "\n",
        "    harmonic, percussive = librosa.effects.hpss(audio)\n",
        "\n",
        "    return {\n",
        "        \"harmonic_energy\": float(np.mean(harmonic**2)),\n",
        "        \"percussive_energy\": float(np.mean(percussive**2))\n",
        "    }\n",
        "\n",
        "# func to extract tempo\n",
        "# speed of the music\n",
        "def extract_tempo_single(entry):\n",
        "    audio = entry[\"audio\"][\"array\"]\n",
        "    sr = entry[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
        "\n",
        "    return {\n",
        "        \"tempo\": float(tempo)\n",
        "    }\n",
        "\n",
        "# func to extract spectral rolloff\n",
        "# where most sound energy ends\n",
        "def extract_spectral_rolloff_single(entry):\n",
        "    audio = entry[\"audio\"][\"array\"]\n",
        "    sr = entry[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
        "\n",
        "    return {\n",
        "        \"rolloff_mean\": float(np.mean(rolloff)),\n",
        "        \"rolloff_var\": float(np.var(rolloff))\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V41M-1RsuQLC"
      },
      "source": [
        "**(ADV) Analyze correlation of the features (5 points)**\n",
        "\n",
        "Plot correlation diagram and conclude which features are too much correlated and could be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-4f605lumOy"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "mfcc_features = extract_mfcc_features(filtered_dataset, n_mfcc=5)\n",
        "feature_df = pd.DataFrame(mfcc_features)\n",
        "\n",
        "corr = feature_df.corr()\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "sns.heatmap(corr, cmap='coolwarm', annot=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnLnepWN7Weh"
      },
      "source": [
        "**(ADV) Analyze feature importance (5 points)**\n",
        "\n",
        "Analyze the importance of each feature used in the model to understand which variables have the greatest impact on predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmI2jGlx7V5y"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "\n",
        "X = feature_df.drop('genre', axis=1)\n",
        "y = feature_df['genre']\n",
        "\n",
        "f_scores, p_values = f_classif(X, y)\n",
        "\n",
        "anova_df = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'f_score': f_scores,\n",
        "    'p_value': p_values\n",
        "}).sort_values('f_score', ascending=False)\n",
        "\n",
        "print(anova_df)\n",
        "\n",
        "# Interpretation:\n",
        "# High F-score → feature distinguishes genres well → important\n",
        "# High p-value (> 0.05) → not statistically useful → candidate for removal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr = feature_df.corr().abs()\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "df_reduced_corr = feature_df.drop(columns=to_drop)\n",
        "df_reduced_corr\n",
        "# drop highly correlated features but i found out that features are not that high correlated"
      ],
      "metadata": {
        "id": "14IHgjxiFJVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePJZimJ931C3"
      },
      "outputs": [],
      "source": [
        "corr_matrix = feature_df.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [col for col in upper.columns if any(upper[col] > 0.85)]\n",
        "df_reduced_corr = feature_df.drop(columns=to_drop)\n",
        "df_reduced_corr\n",
        "# tried another way to drop features with more than 0.85 correlation but end up with same features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHN4fJqH4JSO"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = df_reduced_corr.drop(columns=['genre'])\n",
        "y = df_reduced_corr['genre']\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "low_imp = importances[importances < 0.01].index.tolist()\n",
        "\n",
        "df_final = df_reduced_corr.drop(columns=low_imp)\n",
        "\n",
        "df_final\n",
        "# another way of removing less important features using random forest trained on the features determend less importance ones"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ANOVA → all features useful → keep all\n",
        "# Correlation filtering → no redundancy → keep all\n",
        "# RandomForest importance → no low-importance → keep all"
      ],
      "metadata": {
        "id": "yGJqeYQ8Fgyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juTZFIrQusSv"
      },
      "source": [
        "### Train a classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khUd4Trwuy4A"
      },
      "source": [
        "Let's import all necessary functions and classes from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iL_rApSuwcC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uviJSul7jz8s"
      },
      "outputs": [],
      "source": [
        "# include here any additional libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold, cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSfXqSvvI90"
      },
      "source": [
        "Function to extract features dataset from initial audio dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcM37CK3u-wE"
      },
      "outputs": [],
      "source": [
        "# changed the func to one that suits my code\n",
        "def prepare_dataset(dataset, n_mfcc):\n",
        "    mfcc_list = extract_mfcc_features(dataset, n_mfcc)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # Loop over dataset and MFCC features at the same time, so that we collect all features needed\n",
        "    for entry, mfcc_feats in tqdm(zip(dataset, mfcc_list), total=len(dataset), desc=\"Preparing Dataset\"):\n",
        "        row = dict(mfcc_feats)   # start with MFCC features\n",
        "\n",
        "        # Add RMS features\n",
        "        rms_dict = extract_rms(entry)\n",
        "        row.update(rms_dict)\n",
        "\n",
        "        # Add Spectral Contrast\n",
        "        contrast_dict = extract_spectral_contrast(entry)\n",
        "        row.update(contrast_dict)\n",
        "\n",
        "        # Add ZCR\n",
        "        zcr_dict = extract_zcr(entry)\n",
        "        row.update(zcr_dict)\n",
        "\n",
        "        # Add HPSS\n",
        "        hpss_dict = extract_hpss_single(entry)\n",
        "        row.update(hpss_dict)\n",
        "\n",
        "        # Add Tempo\n",
        "        tempo_dict = extract_tempo_single(entry)\n",
        "        row.update(tempo_dict)\n",
        "\n",
        "        # Add Rolloff\n",
        "        rolloff_dict = extract_spectral_rolloff_single(entry)\n",
        "        row.update(rolloff_dict)\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Remove samples with missing labels instead of just replacing\n",
        "    df = df.dropna(subset=[\"genre\"])\n",
        "\n",
        "    X = df.drop(columns=[\"genre\"])\n",
        "    y = df[\"genre\"]\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuzTWJOyvdZe"
      },
      "source": [
        "Now let's prepare train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c26yICQxvEcH"
      },
      "outputs": [],
      "source": [
        "X, y = prepare_dataset(filtered_dataset, n_mfcc=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHf63HY4JeL6"
      },
      "source": [
        "The best practice is to use a pipeline because it allows us to streamline preprocessing steps (like scaling) and model training into a single workflow. This ensures that all steps are applied consistently during both training and testing, preventing data leakage and simplifying cross-validation and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOxLTs0hu0Sh"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"{pipeline['classifier'].__class__.__name__}: {accuracy:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaCZPoda29qx"
      },
      "source": [
        "**Train other models and compare their performance (10 points)**\n",
        "\n",
        "Try training different models (at least 3) from sklearn and boosting libraries (e.g. Random Forest, SVM, Gradient Boosting, etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Z1G9le4ipQ"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "\n",
        "### RandomForestClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=1000, random_state=42))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"{pipeline['classifier'].__class__.__name__}: {accuracy:.5f}\")\n",
        "\n",
        "### PCA and SVC\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=15)),\n",
        "    ('classifier', SVC(kernel='rbf', C=10, gamma='scale'))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"{pipeline['classifier'].__class__.__name__}: {accuracy:.5f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjN3QMwe3QdE"
      },
      "source": [
        "**(ADV) Choose several best models and perform parametric grid search (10 points)**\n",
        "\n",
        "Choose several of the best-performing models from your previous experiments and tune their hyperparameters using a parametric grid search. Compare the results and discuss which combination performs best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZtVvJIB4jMc"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "### GridSearchCV\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'clf': [SVC()],\n",
        "        'clf__C': [0.1, 1, 10],\n",
        "        'clf__kernel': ['rbf', 'linear']\n",
        "    },\n",
        "    {\n",
        "        'clf': [RandomForestClassifier()],\n",
        "        'clf__n_estimators': [200, 500],\n",
        "        'clf__max_depth': [10, 20]\n",
        "    },\n",
        "]\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best model:\", grid.best_estimator_)\n",
        "print(\"Best score:\", grid.best_score_)\n",
        "\n",
        "# after adding the new features we can see that best score became 0.76"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW5lDpCa3bH7"
      },
      "source": [
        "**(ADV) Make the same experiments with some of the best models using cross validation (10 points)**\n",
        "\n",
        "Repeat the experiments for several of the best-performing models using cross-validation. Compare the results with previous single-split evaluations and discuss the stability of model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4YXcTwX4jui"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestClassifier model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform Cross Validation\n",
        "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "print(scores)\n",
        "\n",
        "# kfolds using random forest after adding new features changed from less 70 ranges to all above 70%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qXf1NMCKjuj"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Assignment specifications and rubric\n",
        "\n",
        "**Please use only the provided dataset: no external datasets are allowed.**\n",
        "\n",
        "**The solution notebook MUST run without errors when executing all cells in google colab.**\n",
        "\n",
        "Focus only on classical machine learning algorithms available in *sklearn* for your analysis, other than artificial neural networks.\n",
        "\n",
        "You are welcome to make models based on artificial neural networks too, but these will not be evaluated.\n",
        "\n",
        "You are free to perform additional experiments or analyses beyond those explicitly mentioned in the exercises.\n",
        "\n",
        "Exercises marked **(ADV)** are slightly more advanced and can be completed to earn extra points.\n",
        "To achieve the minimum passing score, it is sufficient to complete only the base-level exercises.\n",
        "\n",
        "### Main steps\n",
        "\n",
        "1. **Data Visualization and Exploration**:\n",
        "   - Visualize and analyze the dataset to gain insights into the distribution and characteristics of different features.\n",
        "\n",
        "2. **Handle Unlabeled and Irrelevant Data**:\n",
        "   - Investigate the dataset for unlabeled data.\n",
        "   - Filter out irrelevant audios, especially those that are just zero signals or contain no meaningful information.\n",
        "\n",
        "3. **Feature Engineering**:\n",
        "   - Experiment with adding new features or refining existing ones.\n",
        "   - Adjust feature extraction parameters to better capture the characteristics of the audio samples.\n",
        "   - Rank the features and assess how many top-features to use.\n",
        "\n",
        "4. **Apply Different Machine Learning Algorithms**:\n",
        "   - Try various machine learning algorithms (e.g., Random Forest, SVM, Gradient Boosting\\*) to improve performance.\n",
        "   - Evaluate the models not only based on the **average accuracy**, but also consider the confusion matrix along with other **per-class** evaluation.\n",
        "   - Explain which metrics are important for evaluating model performance, based on your findings from the exploratory data analysis.\n",
        "\n",
        "\n",
        "\\**Gradient Boosting is commonly implemented using specialized libraries like XGBoost, CatBoost, or LightGBM.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83LKug0A1Hbr"
      },
      "source": [
        "## Assignment Evaluation Criteria (maximum 100%)\n",
        "\n",
        "1. **Data Handling and Preprocessing (5 + 5 = 10 points)**\n",
        "2. **Exploratory Data Analysis (5 + 5 = 10 points)**\n",
        "3. **Feature Engineering (10 + 10 + 5 + 5 = 30 points)**\n",
        "4. **Model Selection and Comparison (10 + 10 + 10 = 30 points)**\n",
        "5. **Clarity, Creativity, and Originality (20 points)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsHPvh3ODI33"
      },
      "source": [
        "## Assignment submission instructions\n",
        "Complete the assignment in your own copy of the notebook. Ensure that your notebook is runnable and free of errors. Once finished, test out the notebook in Goggle Colab (to make sure it runs in that environment too). Then RUN-ALL the project and export both the **.ipynb** file and the **html** file. Finally, submit both files via the Teams Assignment section.\n",
        "\n",
        "**The deadline is 14 days ahead of the oral exam.**\n",
        "\n",
        "**Actual deadlines are updated in the Teams Assignment Portal.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XCFUq7TyWUT3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}